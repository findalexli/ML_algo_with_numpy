{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNYkDkzFa6da+ltQF8RWos0",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/findalexli/ML_algo_with_numpy/blob/main/Logistic_regression.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "NckhRVc4F2AK"
      },
      "outputs": [],
      "source": [
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris\n",
        "\n",
        "data = load_iris\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "y = np.array(y == 1, dtype = int)"
      ],
      "metadata": {
        "id": "5rCV3-XfHA5O"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 42)\n",
        "\n",
        "scalar = StandardScaler()\n",
        "X_train = scalar.fit_transform(X_train)\n",
        "X_test = scalar.transform(X_test)"
      ],
      "metadata": {
        "id": "o5bJDGiPqPxW"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In logistic regression we find a line or plane, called a decision boundary, that can (almost) linearly separate twoclasses in binary classification."
      ],
      "metadata": {
        "id": "aZQO5SZeyDQw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "$  Log looss = \\frac{1}{\\text{number of points}}  -y log(y') - (1-y)log(1 -y') $ \n",
        "\n",
        "where y' is the predicted probablities \n",
        "\n",
        "$$ Gradient = \\frac_{numer of datapoints} (y - y') * X $$\n"
      ],
      "metadata": {
        "id": "bxvSMLnNLiOD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class LogisitcRegression():\n",
        "    \"\"\"\n",
        "    cls = LogisitcRegression()\n",
        "    cls.fit(X, y)\n",
        "    cls.predict(X_test)\n",
        "    \"\"\"\n",
        "    def __init__(self, learning_rate = 0.001, max_epochs = 1000, eps = 1e-7, fit_intercept=True):\n",
        "        self.learning_rate = learning_rate\n",
        "        self.max_epochs = max_epochs\n",
        "        self.eps = eps\n",
        "        self.fit_intercept = fit_intercept\n",
        "\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        if self.fit_intercept:\n",
        "            X = self._add_intercept(X)\n",
        "        n, dim = X.shape\n",
        "        self.theta = np.zeros(dim)\n",
        "        hist_loss = []\n",
        "        for iter_idx in range(self.max_epochs):\n",
        "            prev_theta = self.theta \n",
        "            prob = _sigmoid(np.dot(X, self.theta))\n",
        "            loss = (-y * np.log(prob) - (1 - y) * np.log(1 - prob)).mean()\n",
        "            hist_loss.append(loss)\n",
        "            \n",
        "            grad = (1/n) * np.dot(X.T, (prob - y))\n",
        "            if iter_idx % 1000 == 0:\n",
        "                print(f\"Loss = {loss}\")\n",
        "                print(f\"Grad = {grad}\")\n",
        "\n",
        "\n",
        "            self.theta -= grad * self.learning_rate\n",
        "            # if iter_idx > 10 and np.linalg.norm(prev_theta - self.theta) < self.eps:\n",
        "            #     print('Early stopping')\n",
        "            #     break\n",
        "\n",
        "    def predict(self, X_test, threshold = 0.5):\n",
        "        if self.fit_intercept:\n",
        "            X_test = self._add_intercept(X_test)\n",
        "\n",
        "        bool_array =  _sigmoid(np.dot(X_test, self.theta)) >= threshold\n",
        "        return bool_array.astype(int)\n",
        "\n",
        "    def _add_intercept(self, X):\n",
        "        intercept_term = np.ones(len(X))\n",
        "        return np.column_stack((intercept_term, X))\n",
        "\n",
        "def _sigmoid(x):\n",
        "    return 1 / (1 + np.exp(-x))\n"
      ],
      "metadata": {
        "id": "Xj5I9U9DHo5f"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "cls = LogisitcRegression()\n",
        "\n",
        "cls.fit(X_train, y_train)\n",
        "preds = cls.predict(X_test)\n",
        "classification_report(preds, y_test)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "id": "H-_TTT0MOWXm",
        "outputId": "cd6f9a81-9a83-4f2e-81bf-ed46e0b5b49b"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss = 0.6931471805599453\n",
            "Grad = [ 0.15833333 -0.04595683  0.22225439 -0.10079432 -0.06321999]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'              precision    recall  f1-score   support\\n\\n           0       0.71      0.83      0.77        18\\n           1       0.67      0.50      0.57        12\\n\\n    accuracy                           0.70        30\\n   macro avg       0.69      0.67      0.67        30\\nweighted avg       0.70      0.70      0.69        30\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "F1 = precision * recall / (precision + recall)"
      ],
      "metadata": {
        "id": "gKuyRrFNtiGJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import roc_auc_score\n",
        "roc_auc_score(preds, y_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ofPRg1WIYpvl",
        "outputId": "49c275f6-b2f9-4891-bcd1-95413370eac3"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.6666666666666666"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The ROC curve is a plot of the true positive rate against the false positive rate at various threshold levels. The\n",
        "ROC is a probability curve and the area under the curve can be thought of as the degree of separability\n",
        "between the two classes.\n",
        "\n",
        "Where precision is defined by P = TP / (TP + FP) which means every time we said an observation was True, was\n",
        "it actually True and recall is defined by R = TP / (TP + FN) which means out of all the True observations, how\n",
        "many did we get.\n",
        "\n",
        "ROC curves can sometimes look like the model is performing well for very imbalanced datasets even if it is\n",
        "misclassifying most of the minority class. However, the precision-recall curve can specifically handle\n",
        "imbalanced datasets, thus it is better in these situations."
      ],
      "metadata": {
        "id": "vxuRc3Q2yrY1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Extend to multi class\n",
        "\n",
        "1. one vs all others\n",
        "2. use softmax function instead of the sigmoid function $$\\frac{e^{Z_i}{\\sum e^ e^{Z_i}$$"
      ],
      "metadata": {
        "id": "1iSJ2LY9y7zO"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "McRmSSXlzGel"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}